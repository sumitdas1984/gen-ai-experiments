{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 1: Install Dependencies\n",
    "# # Run this cell first to ensure all necessary libraries are installed.\n",
    "# # If you're running this in a fresh environment or devcontainer,\n",
    "# # ensure these are installed in your active Python environment.\n",
    "# !pip install -qU langchain-community pypdf chromadb langchain-groq python-dotenv sentence-transformers langchain-chroma\n",
    "\n",
    "# print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Environment Setup ---\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys from environment variables\n",
    "# IMPORTANT: Never hardcode your API keys directly in the script for production!\n",
    "# Create a .env file in the same directory as this script with:\n",
    "# GROQ_API_KEY=\"your_groq_api_key_here\"\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "PDF_DIRECTORY = \"./documents\" # Create this directory and place your PDFs here\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "COLLECTION_NAME = \"pdf_rag_collection\"\n",
    "GROQ_MODEL_NAME = \"llama3-8b-8192\" # e.g., \"mixtral-8x7b-32768\", \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configuration for Hugging Face Embeddings model\n",
    "HF_EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load PDF Documents from Directory ---\n",
    "\n",
    "def load_pdf_documents_from_directory(directory_path: str):\n",
    "    \"\"\"Loads all PDF documents from a specified directory.\"\"\"\n",
    "    print(f\"Loading documents from directory: {directory_path}\")\n",
    "    all_documents = []\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not Path(directory_path).exists():\n",
    "        Path(directory_path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Created directory: {directory_path}. Please place your PDF documents here.\")\n",
    "        return None # Return None as no docs are present yet\n",
    "\n",
    "    pdf_files = glob.glob(os.path.join(directory_path, \"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {directory_path}. Please ensure PDFs are present.\")\n",
    "        return None\n",
    "\n",
    "    for file_path in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading PDF {file_path}: {e}\")\n",
    "    \n",
    "    if not all_documents:\n",
    "        print(\"No documents were successfully loaded. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Successfully loaded a total of {len(all_documents)} pages from {len(pdf_files)} PDF files.\")\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Split Document into Chunks ---\n",
    "\n",
    "def split_documents(documents, chunk_size: int, chunk_overlap: int):\n",
    "    \"\"\"Splits loaded documents into smaller, manageable chunks.\"\"\"\n",
    "    print(f\"Splitting documents into chunks (size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Created {len(chunks)} chunks.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Create Embeddings (Changed to Hugging Face) ---\n",
    "def create_embeddings(model_name: str):\n",
    "    \"\"\"Initializes Hugging Face Embeddings model.\"\"\"\n",
    "    print(f\"Initializing Hugging Face Embeddings (model: {model_name})...\")\n",
    "    # HuggingFaceEmbeddings will download the model the first time it's used\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'} # Use 'cuda' if you have a GPU\n",
    "    )\n",
    "    print(\"Hugging Face Embeddings initialized.\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Store Chunks and Embeddings in ChromaDB ---\n",
    "def create_vector_store(chunks, embeddings, collection_name: str):\n",
    "    \"\"\"Creates and persists a ChromaDB vector store from document chunks and embeddings.\"\"\"\n",
    "    print(f\"Creating/Loading ChromaDB vector store (collection: {collection_name})...\")\n",
    "    # Using a persistent client to store data in a local directory 'chroma_db'\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=\"./chroma_db\" # Data will be saved here\n",
    "    )\n",
    "    # The .persist() method is often not needed with recent langchain-chroma versions\n",
    "    # when persist_directory is provided, as persistence is handled implicitly.\n",
    "    # vectorstore.persist() # Removed this line\n",
    "    print(\"ChromaDB vector store created and persisted.\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Set up Retriever ---\n",
    "def setup_retriever(vectorstore):\n",
    "    \"\"\"Converts the vector store into a retriever.\"\"\"\n",
    "    print(\"Setting up retriever...\")\n",
    "    # 'k' parameter controls how many top-k relevant documents are retrieved\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    print(\"Retriever set up.\")\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Initialize Groq LLM ---\n",
    "def initialize_groq_llm(api_key: str, model_name: str):\n",
    "    \"\"\"Initializes the Groq LLM.\"\"\"\n",
    "    print(f\"Initializing Groq LLM (model: {model_name})...\")\n",
    "    llm = ChatGroq(\n",
    "        temperature=0.0, # Adjust temperature for creativity vs. factual accuracy\n",
    "        groq_api_key=api_key,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    print(\"Groq LLM initialized.\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Build the RAG Chain ---\n",
    "def build_rag_chain(llm, retriever):\n",
    "    \"\"\"Builds the RAG (Retrieval Augmented Generation) chain.\"\"\"\n",
    "    print(\"Building RAG chain...\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an AI assistant for answering questions based on the provided context only. \"\n",
    "                   \"If the answer is not in the context, clearly state that you don't have enough information.\\n\\n\"\n",
    "                   \"Context: {context}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    print(\"RAG chain built successfully.\")\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from directory: ./documents\n",
      "Loaded 7 pages from ./documents/singapore-court-case1.pdf\n",
      "Loaded 5 pages from ./documents/singapore-court-case2.pdf\n",
      "Successfully loaded a total of 12 pages from 2 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# Load and split the documents from the directory\n",
    "documents = load_pdf_documents_from_directory(PDF_DIRECTORY)\n",
    "if documents is None:\n",
    "    print(\"Exiting due to PDF loading error or no documents found.\")\n",
    "    exit() # Use exit() for a script to stop execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks (size=1000, overlap=200)...\n",
      "Created 58 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(documents, CHUNK_SIZE, CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Hugging Face Embeddings (model: sentence-transformers/all-MiniLM-L6-v2)...\n",
      "Hugging Face Embeddings initialized.\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings (Now using Hugging Face model name)\n",
    "embeddings = create_embeddings(HF_EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating/Loading ChromaDB vector store (collection: pdf_rag_collection)...\n",
      "ChromaDB vector store created and persisted.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = create_vector_store(chunks, embeddings, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up retriever...\n",
      "Retriever set up.\n"
     ]
    }
   ],
   "source": [
    "# Setup retriever\n",
    "retriever = setup_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Groq LLM (model: llama3-8b-8192)...\n",
      "Groq LLM initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = initialize_groq_llm(groq_api_key, GROQ_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building RAG chain...\n",
      "RAG chain built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Build RAG chain\n",
    "rag_chain = build_rag_chain(llm, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      "According to the provided context, the penalty rule was developed in relation to the very specific situation of enforcement of penal bonds. However, the context does not explicitly state the content of the penalty rule. It only mentions that the CA did not agree with the historical approach adopted by the court in Andrews and that the penalty rule would be confined to the sphere of secondary obligations, without interfering with the primary obligations between the contracting parties.\n",
      "\n",
      "--- Sources (from retrieved chunks) ---\n",
      "Document 1 (File: singapore-court-case1.pdf, Page: 2):\n",
      "  Content snippet: CA did not agree with the historical approach adopted by the court in Andrews.2 The penalty rule was \n",
      "developed in relation to the very specific situation of enforcement of penal bonds;3 there was no ...\n",
      "Document 2 (File: singapore-court-case1.pdf, Page: 2):\n",
      "  Content snippet: CA did not agree with the historical approach adopted by the court in Andrews.2 The penalty rule was \n",
      "developed in relation to the very specific situation of enforcement of penal bonds;3 there was no ...\n",
      "Document 3 (File: singapore-court-case1.pdf, Page: 2):\n",
      "  Content snippet: CA did not agree with the historical approach adopted by the court in Andrews.2 The penalty rule was \n",
      "developed in relation to the very specific situation of enforcement of penal bonds;3 there was no ...\n",
      "Document 4 (File: singapore-court-case1.pdf, Page: 1):\n",
      "  Content snippet: the court had to consider whether to follow the developments in the penalty rule in Andrews and \n",
      "Cavendish. \n",
      " \n",
      "1 A contract for differences is one in which a party pays another the difference between ...\n",
      "Document 5 (File: singapore-court-case1.pdf, Page: 1):\n",
      "  Content snippet: the court had to consider whether to follow the developments in the penalty rule in Andrews and \n",
      "Cavendish. \n",
      " \n",
      "1 A contract for differences is one in which a party pays another the difference between ...\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What is the content of the penalty rule?\"\n",
    "\n",
    "# Invoke the RAG chain with the user's question\n",
    "response = rag_chain.invoke({\"input\": user_question})\n",
    "\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(response[\"answer\"])\n",
    "\n",
    "print(\"\\n--- Sources (from retrieved chunks) ---\")\n",
    "if response[\"context\"]:\n",
    "    for i, doc in enumerate(response[\"context\"]):\n",
    "        page_number = doc.metadata.get('page', 'N/A')\n",
    "        source_file = os.path.basename(doc.metadata.get('source', 'N/A'))\n",
    "        print(f\"Document {i+1} (File: {source_file}, Page: {page_number}):\")\n",
    "        content_snippet = str(doc.page_content)[:200]\n",
    "        print(f\"  Content snippet: {content_snippet}...\")\n",
    "    print(\"--------------------------------------\")\n",
    "else:\n",
    "    print(\"No relevant context found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
